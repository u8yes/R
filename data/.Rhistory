remotes::install_github('haven-jeon/KoNLP', upgrade = "never", INSTALL_opts=c("--no-multiarch"), force=TRUE)
library(KoNLP) # Korean Natural Language Process
# KoNLP 테스트 예제
sentence <- '아버지가 방에 스르륵 들어가신다.'
extractNoun(sentence)
install.packages(c("wordcloud","tm"))
library(wordcloud); library(tm)
# (2) 텍스트 자료 가져오기
facebook <- file("facebook_bigdata.txt", encoding = "UTF-8")
facebook
facebook_data <- readLines(facebook) # 줄 단위 데이터 생성 # 패러그래프 단위로 끊어줌
head(facebook_data) # 앞부분 6줄 보기 - 줄 단위 데이터 생성
str(facebook_data) # chr [1:76]
close(facebook)
# (3) 세종 사전에 신규 단어 추가(term에 추가)
# ncn - 명사형을 표현해주는 식별자
userDic <- data.frame(term=c("R 프로그래밍","페이스북","소셜네트워크","얼죽아"), tag='ncn')
# - 신규 단어 사전 추가 함수 # sejong단어에 신규 단어를 추가하고 싶을 때
# 시간이 좀 소요됨.
buildDictionary(ext_dic = 'sejong', user_dic = userDic)
# (3) 세종 사전에 신규 단어 추가(term에 추가)
# ncn - 명사형을 표현해주는 식별자
userDic <- data.frame(term=c("R 프로그래밍","페이스북","소셜네트워크","얼죽아"), tag='ncn')
# - 신규 단어 사전 추가 함수 # sejong단어에 신규 단어를 추가하고 싶을 때
# 시간이 좀 소요됨.
buildDictionary(ext_dic = 'sejong', user_dic = userDic)
# - R 제공 함수로 단어 추출하기 - Sejong 사전에 등록된 신규 단어 테스트
paste(extractNoun("홍길동은 얼죽아를 최애로 생각하는, 빅데이터에 최대 관심을 가지고 있으면서, 페이스북이나 소셜네트워크로부터 생성 수집되어진 빅데이터 분석에 아주 많은 관심을 가지고 있어요."),collapse=" ")
# 단어 추출을 위한 사용자 정의 함수 정의하기
# (1) 사용자 정의 함수 작성
#     - [문자변환]->[명사 단어 추출]->[공백으로 합침]
exNouns <- function(x){
paste(extractNoun(as.character(x)), collapse = " ")
}
# - R 제공 함수로 단어 추출하기 - Sejong 사전에 등록된 신규 단어 테스트
paste(extractNoun("홍길동은 얼죽아를 최애로 생각하는, 빅데이터에 최대 관심을 가지고 있으면서, 페이스북이나 소셜네트워크로부터 생성 수집되어진 빅데이터 분석에 아주 많은 관심을 가지고 있어요."),collapse=" ")
# (1) 패키지 설치 및 준비
getwd()
setwd("D:/heaven_dev/workspaces/R/data")
install.packages("rJava") # r에서 JAVA가 가능하게 해주는 패키지
Sys.setenv(JAVA_HOME='C:/Program Files/Java/jdk-11.0.16.1')
library(rJava)
# 아래 패키지를 설치 후
install.packages("multilinguer")
library(multilinguer)
# 의존성을 설치 한다.
install.packages(c('stringr', 'hash', 'tau', 'Sejong', 'RSQLite', 'devtools'), type = "binary") # Sejong 사전에서 연동한 후 동작이 되는 패키지(연동 패키지 RSQLite) # c()를 이용해서 한꺼번에 패키지 설치 가능하다.
library(stringr); library(hash); library(tau);
library(Sejong); library(RSQLite); library(devtools);
# Git hub로 설치 한다.
install.packages("remotes")
library(remotes)
remotes::install_github('haven-jeon/KoNLP', upgrade = "never", INSTALL_opts=c("--no-multiarch"), force=TRUE)
library(KoNLP) # Korean Natural Language Process
# KoNLP 테스트 예제
sentence <- '아버지가 방에 스르륵 들어가신다.'
extractNoun(sentence)
install.packages(c("wordcloud","tm"))
library(wordcloud); library(tm)
# (2) 텍스트 자료 가져오기
facebook <- file("facebook_bigdata.txt", encoding = "UTF-8")
facebook
facebook_data <- readLines(facebook) # 줄 단위 데이터 생성 # 패러그래프 단위로 끊어줌
head(facebook_data) # 앞부분 6줄 보기 - 줄 단위 데이터 생성
str(facebook_data) # chr [1:76]
close(facebook)
# (3) 세종 사전에 신규 단어 추가(term에 추가)
# ncn - 명사형을 표현해주는 식별자
userDic <- data.frame(term=c("R 프로그래밍","페이스북","소셜네트워크","얼죽아"), tag='ncn')
# - 신규 단어 사전 추가 함수 # sejong단어에 신규 단어를 추가하고 싶을 때
# 시간이 좀 소요됨.
buildDictionary(ext_dic = 'sejong', user_dic = userDic)
# - R 제공 함수로 단어 추출하기 - Sejong 사전에 등록된 신규 단어 테스트
paste(extractNoun("홍길동은 얼죽아를 최애로 생각하는, 빅데이터에 최대 관심을 가지고 있으면서, 페이스북이나 소셜네트워크로부터 생성 수집되어진 빅데이터 분석에 아주 많은 관심을 가지고 있어요."),collapse=" ")
# 단어 추출을 위한 사용자 정의 함수 정의하기
# (1) 사용자 정의 함수 작성
#     - [문자변환]->[명사 단어 추출]->[공백으로 합침]
exNouns <- function(x){
paste(extractNoun(as.character(x)), collapse = " ")
}
# - R 제공 함수로 단어 추출하기 - Sejong 사전에 등록된 신규 단어 테스트
paste(extractNoun("홍길동은 얼죽아를 최애로 생각하는, 빅데이터에 최대 관심을 가지고 있으면서, 페이스북이나 소셜네트워크로부터 생성 수집되어진 빅데이터 분석에 아주 많은 관심을 가지고 있어요."),collapse=" ")
# 단어 추출을 위한 사용자 정의 함수 정의하기
# (1) 사용자 정의 함수 작성
#     - [문자변환]->[명사 단어 추출]->[공백으로 합침]
exNouns <- function(x){
paste(extractNoun(as.character(x)), collapse = " ")
}
# (2) exNouns 함수 이용 단어 추출
facebook_nouns <- sapply(facebook_data, exNouns) # 명사 단어 추출.
facebook_nouns[1] # 단어가 추출된 첫 줄 보기
# (5) 추출된 단어 대상 전처리
#  단계1: 추출된 단어 이용 말뭉치(Corpus) 생성
myCorpus <- Corpus(VectorSource(facebook_nouns))
myCorpus
# (1) 패키지 설치 및 준비
getwd()
setwd("D:/heaven_dev/workspaces/R/data")
install.packages("rJava") # r에서 JAVA가 가능하게 해주는 패키지
Sys.setenv(JAVA_HOME='C:/Program Files/Java/jdk-11.0.16.1')
library(rJava)
# 아래 패키지를 설치 후
install.packages("multilinguer")
# (1) 패키지 설치 및 준비
getwd()
setwd("D:/heaven_dev/workspaces/R/data")
install.packages("rJava") # r에서 JAVA가 가능하게 해주는 패키지
Sys.setenv(JAVA_HOME='C:/Program Files/Java/jdk-11.0.16.1')
library(rJava)
# 아래 패키지를 설치 후
install.packages("multilinguer")
library(multilinguer)
# 의존성을 설치 한다.
install.packages(c('stringr', 'hash', 'tau', 'Sejong', 'RSQLite', 'devtools'), type = "binary") # Sejong 사전에서 연동한 후 동작이 되는 패키지(연동 패키지 RSQLite) # c()를 이용해서 한꺼번에 패키지 설치 가능하다.
library(stringr); library(hash); library(tau);
library(Sejong); library(RSQLite); library(devtools);
# Git hub로 설치 한다.
install.packages("remotes")
library(remotes)
remotes::install_github('haven-jeon/KoNLP', upgrade = "never", INSTALL_opts=c("--no-multiarch"), force=TRUE)
library(KoNLP) # Korean Natural Language Process
# KoNLP 테스트 예제
sentence <- '아버지가 방에 스르륵 들어가신다.'
# KoNLP 테스트 예제
sentence <- '아버지가 방에 스르륵 들어가신다.'
extractNoun(sentence)
library(KoNLP) # Korean Natural Language Process
# KoNLP 테스트 예제
sentence <- '아버지가 방에 스르륵 들어가신다.'
extractNoun(sentence)
install.packages(c("wordcloud","tm"))
library(wordcloud); library(tm)
# (2) 텍스트 자료 가져오기
facebook <- file("facebook_bigdata.txt", encoding = "UTF-8")
facebook
facebook_data <- readLines(facebook) # 줄 단위 데이터 생성 # 패러그래프 단위로 끊어줌
head(facebook_data) # 앞부분 6줄 보기 - 줄 단위 데이터 생성
str(facebook_data) # chr [1:76]
close(facebook)
# (3) 세종 사전에 신규 단어 추가(term에 추가)
# ncn - 명사형을 표현해주는 식별자
userDic <- data.frame(term=c("R 프로그래밍","페이스북","소셜네트워크","얼죽아"), tag='ncn')
# - 신규 단어 사전 추가 함수 # sejong단어에 신규 단어를 추가하고 싶을 때
# 시간이 좀 소요됨.
buildDictionary(ext_dic = 'sejong', user_dic = userDic)
# - R 제공 함수로 단어 추출하기 - Sejong 사전에 등록된 신규 단어 테스트
paste(extractNoun("홍길동은 얼죽아를 최애로 생각하는, 빅데이터에 최대 관심을 가지고 있으면서, 페이스북이나 소셜네트워크로부터 생성 수집되어진 빅데이터 분석에 아주 많은 관심을 가지고 있어요."),collapse=" ")
# 단어 추출을 위한 사용자 정의 함수 정의하기
# (1) 사용자 정의 함수 작성
#     - [문자변환]->[명사 단어 추출]->[공백으로 합침]
exNouns <- function(x){
paste(extractNoun(as.character(x)), collapse = " ")
}
# (2) exNouns 함수 이용 단어 추출
facebook_nouns <- sapply(facebook_data, exNouns) # 명사 단어 추출.
facebook_nouns[1] # 단어가 추출된 첫 줄 보기
# (1) 패키지 설치 및 준비
getwd()
setwd("D:/heaven_dev/workspaces/R/data")
install.packages("rJava") # r에서 JAVA가 가능하게 해주는 패키지
# Git hub로 설치 한다.
install.packages("remotes")
library(remotes)
remotes::install_github('haven-jeon/KoNLP', upgrade = "never", INSTALL_opts=c("--no-multiarch"), force=TRUE)
library(KoNLP) # Korean Natural Language Process
# KoNLP 테스트 예제
sentence <- '아버지가 방에 스르륵 들어가신다.'
extractNoun(sentence)
install.packages(c("wordcloud","tm"))
library(wordcloud); library(tm)
# (2) 텍스트 자료 가져오기
facebook <- file("facebook_bigdata.txt", encoding = "UTF-8")
facebook
facebook_data <- readLines(facebook) # 줄 단위 데이터 생성 # 패러그래프 단위로 끊어줌
head(facebook_data) # 앞부분 6줄 보기 - 줄 단위 데이터 생성
str(facebook_data) # chr [1:76]
close(facebook)
# (3) 세종 사전에 신규 단어 추가(term에 추가)
# ncn - 명사형을 표현해주는 식별자
userDic <- data.frame(term=c("R 프로그래밍","페이스북","소셜네트워크","얼죽아"), tag='ncn')
# - 신규 단어 사전 추가 함수 # sejong단어에 신규 단어를 추가하고 싶을 때
# 시간이 좀 소요됨.
buildDictionary(ext_dic = 'sejong', user_dic = userDic)
# - R 제공 함수로 단어 추출하기 - Sejong 사전에 등록된 신규 단어 테스트
paste(extractNoun("홍길동은 얼죽아를 최애로 생각하는, 빅데이터에 최대 관심을 가지고 있으면서, 페이스북이나 소셜네트워크로부터 생성 수집되어진 빅데이터 분석에 아주 많은 관심을 가지고 있어요."),collapse=" ")
# - R 제공 함수로 단어 추출하기 - Sejong 사전에 등록된 신규 단어 테스트
paste(extractNoun("홍길동은 얼죽아를 최애로 생각하는, 빅데이터에 최대 관심을 가지고 있으면서, 페이스북이나 소셜네트워크로부터 생성 수집되어진 빅데이터 분석에 아주 많은 관심을 가지고 있어요."),collapse=" ")
# Git hub로 설치 한다.
install.packages("remotes")
library(remotes)
remotes::install_github('haven-jeon/KoNLP', upgrade = "never", INSTALL_opts=c("--no-multiarch"), force=TRUE)
library(KoNLP) # Korean Natural Language Process
# - R 제공 함수로 단어 추출하기 - Sejong 사전에 등록된 신규 단어 테스트
paste(extractNoun("홍길동은 얼죽아를 최애로 생각하는, 빅데이터에 최대 관심을 가지고 있으면서, 페이스북이나 소셜네트워크로부터 생성 수집되어진 빅데이터 분석에 아주 많은 관심을 가지고 있어요."),collapse=" ")
# 2. 줄 단위 단어 추출
lword <- Map(extractNoun, marketing2)
head(lword)
c1 <- rep(1:10, each=2)
c1 # 1  1  2  2  3  3  4  4  5  5  6  6  7  7  8  8  9  9 10 10
c2 <- rep(c(1, 3, 5, 7, 9), each=4)
c2 # 1 1 1 1 3 3 3 3 5 5 5 5 7 7 7 7 9 9 9 9
c3 <- c(1,1,1,1,3,3,3,3,5,5,6,6,7,7,8,8,9,10,11,12)
c3 # 1  1  1  1  3  3  3  3  5  5  6  6  7  7  8  8  9 10 11 12
c123_df <- data.frame(cbind(c1,c2,c3))
c123_df
str(c123_df) # "data.frame":	20 obs. of  3 variables:
c12_unique <- unique(c123_df[,c("c1","c2")])
c12_unique
c123_unique <- unique(c123_df[,c("c1","c2","c3")])
c123_unique
c123_unique_Last <- unique(c123_df[,c("c1","c2","c3")],fromLast=T)
c123_unique_Last
lword <- unique(lword) # 공백 block 제거
length(lword) # 353
lword <- sapply(lword, unique)
length(lword) # 353
# 텍스트 파일 가져오기와 단어 추출하기
# 1. 텍스트 파일 가져오기
marketing <- file("marketing.txt", encoding = "UTF-8")
marketing2 <- readLines(marketing) # 줄 단위 데이터 생성
marketing2
close(marketing)
# 2. 줄 단위 단어 추출
library(KoNLP)
lword <- Map(extractNoun, marketing2)
head(lword)
head(lword)
length(lword) # 472
View(lword)
View(lword)
lword <- unique(lword) # 공백 block 제거
length(lword) # 353
lword <- sapply(lword, unique)
length(lword) # 353
str(lword) # List of 353
lword <- sapply(lword, unique)
length(lword) # 353
str(lword) # List of 353
# 1) 단어 필터링 함수 정의 - 길이가 2개 이상 4개 이하 사이의 문자 길이로 구성된 단어만 필터링.
filter1 <- function(x){
nchar(x) >= 2 && nchar(x) <= 4 && is.hangul(x) # 한글만을 필터링 하겠다.
}
filter2 <- function(x){
Filter(filter1, x)
}
#  2) 줄 단위로 추출된 단어 전처리
lword <- sapply(lword, filter2) # 단어 길이가 1이하 또는 5 이상인 단어 제거.
lword
# 1) 연관 분석을 위한 패키지 설치
install.packages("arules")
library(arules)
# 2) 트랜잭션 생성
wordtran <- as(lword, "transactions")
wordtran
# 3) 교차표 작성: crossTable() -> 교차테이블 함수를 이용.
wordtable <- crossTable(wordtran)
wordtable
# 1) 실습 파일 가져오기
data <- read.csv("D:/heaven_dev/workspaces/R/data/cleanDescriptive.csv", header = T)
View(data)
head(data) # 변수 확인
# 2) 변수 리코딩
x <- data$level2 # 리코딩 변수 이용(학력수준)
y <- data$pass2  # 리코딩 변수 이용(합격/불합격)
x; y  # 부모학력수준(x) -> 자녀대학진학여부(y)
# 3) 데이터프레임 생성
result <- data.frame(Level=x, Pass=y) # 데이터 프레임 생성 - 데이터 묶음.
dim(result) # 차원보기(248,2)
head(result)
View(result)
# 1) 교차 분할표 작성
table(result) # 빈도보기
# 2) 교차분할표 생성을 위한 패키지 설치
install.packages("gmodels")
library(gmodels)
# 3) 패키지를 이용한 교차 분할표 생성
CrossTable(x, y)
# 교차테이블에 카이검정 적용
CrossTable(x, y, chisq = T) # 카이제곱분석 = TRUE
chisq.test(c(4,6,17,16,8,9))
# 5개 제품의 스포츠 음료에 대한 선호도에 차이가 있는지 검정
data <- textConnection(
"스포츠음료종류  관측도수
1               41
2               30
3               51
4               71
5               61
")
class(data)
x <- read.table(data, header = T) # dataFrame 형태로 변환
x # 스포츠음료종류   관측도수
str(x)
chisq.test(x$관측도수)
data <- read.csv("D:/heaven_dev/workspaces/R/data/cleanDescriptive.csv", header = T)
# 독립변수(x) = 설명변수, 종속변수(y) = 반응 변수 생성
x <- data$level2 # 부모의 학력수준
y <- data$pass2  # 자녀의 대학 진학 여부
CrossTable(x, y, chisq = T)
# 1. 파일 가져오기
data <- read.csv("D:/heaven_dev/workspaces/R/data/homogenity.csv", header = T)
View(data)
table(data$method) # 교육방법
table(data$survey) # 만족도
# 전처리 - 결측치/ method와 survey 변수만 서브셋 생성
data <- subset(data, !is.na(survey), c(method, survey))
data
length(data$survey) # 150
# method2 필드 추가
data$method2[data$method == 1] <- "방법1"
data$method2[data$method == 2] <- "방법2"
data$method2[data$method == 3] <- "방법3"
# survey2 필드 추가
data$survey2[data$survey == 1] <- "1. 매우만족"
data$survey2[data$survey == 2] <- "2. 만족"
data$survey2[data$survey == 3] <- "3. 보통"
data$survey2[data$survey == 4] <- "4. 불만족"
data$survey2[data$survey == 5] <- "5. 매우불만족"
# 3. 교차분할표 작성
table(data$method2, data$survey2) # 교차표 생성 -> table(행, 열)
# 4. 교차분할표 생성
CrossTable(data$method2, data$survey2, chisq = T)
# 5. 동질성 검정 - 모수 특성치에 대한 추론 검정
chisq.test(data$method2, data$survey2)
# 3) 교차표 작성: crossTable() -> 교차테이블 함수를 이용.
wordtable <- crossTable(wordtran)
wordtable
# 4) 단어 간 연관 규칙 산출
tranrules <- apriori(wordtran,parameter=list(support=0.25, conf=0.05))
# 5) 연관 규칙 생성 결과 보기
inspect(tranrules)
# 5) 연관 규칙 생성 결과 보기
inspect(tranrules)
# 4) 단어 간 연관 규칙 산출
tranrules <- apriori(wordtran,parameter=list(support=0.25, conf=0.05))
# 5) 연관 규칙 생성 결과 보기
inspect(tranrules) # 쿠퍼스에서도 쓴 inspect, 트랜젝션에서도 볼 수 있음.
# 연관어 시각화
# 1) 연관 단어 시각화를 위해서 자료 구조 변경
rules <- labels(tranrules, ruleSep=" ") # 연관규칙 레이블을 " "으로 분리
head(rules, 20)
class(rules) # "character"
# 2) 문자열로 묶인 연관 단어를 행렬 구조 변경.
rules <- sapply(rules, strsplit, " ", USE.NAMES = F)
rules
class(rules) # "list"
# 3) 행 단위로 묶어서 matrix로 반환
rulemat <- do.call("rbind", rules)
rulemat
class(rulemat) # "matrix"
# 연관어 시각화를 위한 igraph 패키지 설치
install.packages("igraph")
library(igraph)
# edgelist 보기 - 연관 단어를 정점(Vertex) 형태의 목록 제공
ruleg <- graph.edgelist(rulemat[c(12:59),], directed = F) #[1,]~[11,] "{}" 제외
ruleg
plot.igraph(ruleg,vertex.label=V(ruleg)$name,
vertex.label.cex=1.2, vertex.label.color='black',
vertex.size=20, vertex.color='green',
vertex.frame.color='blue')
class(rules) # "list"
print("Hello, R!!!") # ctrl+Enter
# R Package 보기
dim(available.packages()) # dimension 차원, 18688 패키지 수가 있고 17개의 정보를 가지고 있다.
available.packages()
# 패키지 사용법
install.packages("stringr") # 패키지 설치
library("stringr") # 메모리에 로딩 : "" 생략 가능
# install.packages("ichimoku")
search() # 패키지 메모리 로딩 확인
# 패키지 제거
remove.packages("stringr")
install.packages("stringr")
search()
# R session 보기
sessionInfo()
data()
hist(Nile) # hist - 히스토그램의 약자
# 단계2 : 밀도(density)를 기준으로 히스토그램 그리기
hist(Nile, freq = F) # Boolean형태로 F(alse), 곧 대문자로만 작성해야됨, 소문자로 쓰면 에러가 나게 됨. T는 당연히 True이다. 자바와 이런 차이가 있다. 심지어 순서를 바꿔도 상관없다.
?hist # 기능 설명
# 다차원 리스트를 열 단위로 바인딩
d <- do.call(cbind, multi_list)
d
d <- do.call(rbind, multi_list)
d
class(d) # "matrix"
# 데이터프레임의 부분 객체 만들기 # 전처리에서 가장 많이 사용하는 함수
# 내가 원하는 데이터를 필터하는 용도
x1 <- subset(df, x >= 3) # x가 3이상인 레코드 대상
x1
y1 <- subset(df, y <= 8) # y가 8이하인 레코드 대상
y1
# 연관어 시각화
# 1) 연관 단어 시각화를 위해서 자료 구조 변경
rules <- labels(tranrules, ruleSep=" ") # 연관규칙 레이블을 " "으로 분리
head(rules, 20)
class(rules) # "character"
# 2) 문자열로 묶인 연관 단어를 행렬 구조 변경.
rules <- sapply(rules, strsplit, " ", USE.NAMES = F)
# sapply는 벡터 형식으로 바꿔줌.
rules
class(rules) # "list"
# 3) 행 단위로 묶어서 matrix로 반환
rulemat <- do.call("rbind", rules)
rulemat
class(rulemat) # "matrix"
# edgelist 보기 - 연관 단어를 정점(Vertex) 형태의 목록 제공
ruleg <- graph.edgelist(rulemat[c(12:59),], directed = F) #[1,]~[11,] "{}" 제외
plot.igraph(ruleg,vertex.label=V(ruleg)$name,
vertex.label.cex=1.2, vertex.label.color='black',
vertex.size=20, vertex.color='green',
vertex.frame.color='blue')
# 3) 행 단위로 묶어서 matrix로 반환
rulemat <- do.call("rbind", rules)
rulemat
class(rulemat) # "matrix"
# 연관어 시각화를 위한 igraph 패키지 설치
install.packages("igraph")
library(igraph)
# edgelist 보기 - 연관 단어를 정점(Vertex) 형태의 목록 제공
ruleg <- graph.edgelist(rulemat[c(12:59),], directed = F) #[1,]~[11,] "{}" 제외
ruleg
plot.igraph(ruleg,vertex.label=V(ruleg)$name,
vertex.label.cex=1.2, vertex.label.color='black',
vertex.size=20, vertex.color='green',
vertex.frame.color='blue')
#  (1) 패키지 설치 및 준비
# 실습: 웹 문서 요청과 파싱 관련 패키지 설치 및 로딩
install.packages("httr")
library(httr)
install.packages("XML")
library(XML)
# 실습: 웹 문서 요청
url <- "https://news.daum.net"
web <- GET(url)
web
# 실습: HTML 파싱하기
html <- htmlTreeParse(web, useInternalNodes = T, trim = T, encoding = "utf-8")
rootNode <- xmlRoot(html)
html
# 실습: HTML 파싱하기
html <- htmlTreeParse(web, useInternalNodes = T, trim = T, encoding = "utf-8")
rootNode <- xmlRoot(html)
html
# 실습: 태그 자료 수집하기
news <- xpathSApply(rootNode, "//a[@class = 'link_txt']", xmlValue)
news
# 실습: 자료 전처리하기
# 단계 1: 자료 전처리 - 수집한 문서를 대상으로 불용어 제거
news_pre <- gsub("[\r\n\t]", ' ', news)
news_pre <- gsub('[[:punct:]]', ' ', news_pre)
news_pre <- gsub('[[:cntrl:]]', ' ', news_pre)
# news_pre <- gsub('\\d+', ' ', news_pre)   # corona19(covid19) 때문에 숫자 제거 생략
news_pre <- gsub('[a-z]+', ' ', news_pre)
news_pre <- gsub('[A-Z]+', ' ', news_pre)
news_pre <- gsub('\\s+', ' ', news_pre)
news_pre
# 단계 2: 기사와 관계 없는 'TODAY', '검색어 순위' 등의 내용은 제거
news_data <- news_pre[1:32] # 검색수 만큼 변경
news_data
# 실습: 수집한 자료를 파일로 저장하고 읽기
write.csv(news_data, "D:/heaven_dev/workspaces/R/output/news_data.csv", quote = F)
news_data <- read.csv("D:/heaven_dev/workspaces/R/output/news_data.csv", header = T, stringsAsFactors = F)
str(news_data)
names(news_data) <- c("no", "news_text")
head(news_data)
news_text <- news_data$news_text
news_text
# 실습: 세종 사전에 단어 추가
user_dic <- data.frame(term = c("이태원역", "탄도미사일", "이태원"), tag = 'ncn')
buildDictionary(ext_dic = 'sejong', user_dic = user_dic)
# 실습: 단어 추출 사용자 함수 정의하기
# 단계 1: 사용자 정의 함수 작성
exNouns <- function(x) { paste(extractNoun(x), collapse = " ")}
# 단계 2: exNouns()  함수를 이용하어 단어 추출
news_nouns <- sapply(news_text, exNouns)
news_nouns
# 단계 3: 추출 결과 확인
str(news_nouns)
# 실습: 말뭉치 생성과 집계 행렬 만들기
# 단계 1: 추출된 단어를 이용한 말뭉치(corpus) 생성
newsCorpus <- Corpus(VectorSource(news_nouns))
newsCorpus
#<<SimpleCorpus>>
#Metadata:  corpus specific: 1, document level (indexed): 0
#Content:  documents: 32
inspect(newsCorpus)
# 단계 2: 단어 vs 문서 집계 행렬 만들기
# 한글 2~8 음절 단어 대상 단어/문서 집계 행렬
TDM <- TermDocumentMatrix(newsCorpus, control = list(wordLengths = c(4, 16)))
TDM
# 단계 3: matrix 자료구조를 data.frame 자료구조로 변경
tdm.df <- as.data.frame(as.matrix(TDM))
dim(tdm.df)
# 실습: 단어 출현 빈도수 구하기
wordResult <- sort(rowSums(tdm.df), decreasing = TRUE)
wordResult[1:10]
# 실습: 단어 구름 생성
# 단계 1: 패키지 로딩과 단어 이름 추출
library(wordcloud)
myNames <- names(wordResult)
myNames
# 단계 2: 단어와 단어 빈도수 구하기
df <- data.frame(word = myNames, freq = wordResult)
head(df)
# 단계 3: 단어 구름 생성
pal <- brewer.pal(12, "Paired")
x11()
wordcloud(df$word, df$freq, min.freq = 2,
random.order = F, scale = c(4, 0.7),
rot.per = .1, colors = pal)
